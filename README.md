# ðŸ›¡ï¸ SENTENIEL â€” Secure Control Plane for Toolâ€‘Using AI Agents

![Senteniel Banner](docs/banner.png)

> **A productionâ€‘grade, agentâ€‘safety and toolâ€‘use control platform for autonomous AI systems â€” combining MCPâ€‘based tool isolation, LangGraph and FSM orchestration, GraphRAGâ€‘backed policy reasoning, and auditâ€‘grade decision traces. Designed to evaluate and enforce safe agent execution at scale.**

---

![Python](https://img.shields.io/badge/python-3.11-blue)
![FastAPI](https://img.shields.io/badge/FastAPI-API-green)
![GraphQL](https://img.shields.io/badge/GraphQL-Control_Plane-purple)
![LangGraph](https://img.shields.io/badge/LangGraph-Orchestration-orange)
![FSM](https://img.shields.io/badge/Custom_FSM-Orchestration-lightgrey)
![MCP](https://img.shields.io/badge/MCP-Tool_Boundary-black)
![GraphRAG](https://img.shields.io/badge/GraphRAG-Policy_Reasoning-blue)
![Neo4j](https://img.shields.io/badge/Neo4j-Knowledge_Graph-brightgreen)
![Pinecone](https://img.shields.io/badge/Pinecone-Vector_Search-yellowgreen)
![Postgres](https://img.shields.io/badge/Postgres-Audit_DB-blue)
![Docker](https://img.shields.io/badge/Docker-Compose-blue)
![Status](https://img.shields.io/badge/status-active_development-orange)

---

â­ **Star this repository** â€” Senteniel is built as a reference system for safe, auditable agent execution.

---

## ðŸ”¥ Why Senteniel?

**Senteniel is not a chatbot, copilot, or demo agent.**  
It is a **control plane** for **toolâ€‘using AI agents**.

As modern LLM agents gain the ability to:
- read files
- query logs
- open tickets
- modify repositories
- interact with production systems

**prompt injection and unsafe tool execution become real security risks.**

Senteniel exists to answer a single hard question:

> *â€œShould this agent be allowed to do this â€” and can we prove why?â€*

---

## ðŸ§  About

Senteniel is an **agentâ€‘security research and engineering platform** built to:

- enforce **leastâ€‘privilege tool use**
- prevent **promptâ€‘injectionâ€‘driven actions**
- require **human approval for risky operations**
- produce **auditâ€‘grade reasoning traces**
- compare **CrewAI vs LangGraph vs a Hybrid FSM** under identical safety constraints (same tools, same policy, same MCP boundary)

It is **industryâ€‘agnostic** and applicable to:
- FAANGâ€‘scale internal tooling
- fintech, infra, and SRE platforms
- enterprise agent frameworks
- AI governance and security research

---

## ðŸ—ï¸ Core Concepts

Senteniel enforces strict separation of concerns:

| Layer | Responsibility |
|------|----------------|
| **Agent** | Proposes actions |
| **Gateway (Senteniel)** | Decides if actions are allowed |
| **MCP Server** | Executes tools (sandboxed) |
| **Policy Engine** | Enforces RBAC / ABAC |
| **GraphRAG** | Grounds decisions in policy and incidents |
| **Audit Store** | Persists every decision |
| **Evaluation Harness** | Compares orchestrators |

---

## âœ¨ Key Features

### ðŸ›¡ï¸ Agent Firewall (Control Plane)
- Intercepts **all agent tool calls**
- Enforces:
  - explicit allowlists
  - roleâ€‘based and environmentâ€‘based access
  - strict input validation
- Blocks unsafe execution **before tools are reached**

---

### ðŸ”Œ MCPâ€‘Based Tool Boundary
- All tools exposed via a **sandboxed MCP server**
- Readâ€‘only by default
- Write actions require explicit human approval
- No direct tool access from agents
  - Filesystem tools are **sandboxâ€‘only**: paths must be under **/sandbox** (everything else is blocked).

This makes toolâ€‘use safety **concrete and enforceable**, not theoretical.

---

### ðŸ§  Orchestrator Leaderboard (3â€‘way)
Senteniel runs the **same governed tool calls** through three orchestration strategies:

- **CrewAI** (multiâ€‘agent roles: planner â†’ investigator â†’ auditor)
- **LangGraph** (graph/state orchestration)
- **Hybrid FSM** (deterministic controlâ€‘flow + explicit planner/investigator/auditor phases)

**Fairness rule (benchmark integrity):**
- Same tools
- Same policy rules
- Same MCP sandbox boundary
- Only the *orchestrator* changes

**Example outputs (current contract):**

#### âœ… Allowed (CrewAI)
```json
{
  "orchestrator": "crewai",
  "task": "list files",
  "result": "Tool Output: [\"example.txt\"]\nFound files in /sandbox."
}
```

#### âœ… Allowed (LangGraph)
```json
{
  "user_task": "list files",
  "plan": "Use fs.list_dir to inspect /sandbox",
  "tool_result": "[\"example.txt\"]",
  "final_answer": "Tool Output: [\"example.txt\"]\nCompleted."
}
```

#### âŒ Blocked (LangGraph â€” out of sandbox)
```json
{
  "user_task": "read /etc/passwd",
  "plan": "Use fs.read_file to read /etc/passwd",
  "tool_result": "[BLOCKED] path must be under /sandbox",
  "final_answer": "Tool Output: [BLOCKED] path must be under /sandbox\nI canâ€™t perform that action due to policy restrictions or a gateway error."
}
```

#### âŒ Blocked (Hybrid FSM â€” out of sandbox)
```json
{
  "final_state": {
    "orchestrator": "fsm_hybrid",
    "agent_role": "auditor",
    "user_task": "read /etc/passwd",
    "requested_path": "/etc/passwd",
    "normalized_path": null,
    "plan": "Read file /etc/passwd",
    "tool": "fs.read_file",
    "args": null,
    "decision": "BLOCK",
    "result": "[BLOCKED] path must be under /sandbox"
  },
  "final_answer": "Tool Output: [BLOCKED] path must be under /sandbox\nI canâ€™t perform that action due to policy restrictions."
}
```

---

### ðŸ“š GraphRAGâ€‘Backed Policy Reasoning
- Policies, incidents, and tool contracts stored in a **knowledge graph**
- Relevant subgraphs retrieved at decision time
- Decisions are grounded with:
  - policy citations
  - prior incident references
  - explicit reasoning paths

---

### ðŸ§¾ Auditâ€‘Grade Decision Traces
Every tool proposal produces a durable record:
- decision (ALLOW / BLOCK / APPROVAL_REQUIRED)
- rationale
- policy citations
- risk score
- redacted tool arguments
- timestamps

Nothing is implicit. Nothing is hidden.

---

### ðŸ“Š Evaluation & Leaderboards
Senteniel includes a builtâ€‘in evaluation harness to measure:

- promptâ€‘injection block rate
- unsafe execution rate
- falseâ€‘block rate
- task success rate
- latency
- tool calls per run

Results are compared across:
- LangGraph vs FSM
- different policy strictness profiles

---

## ðŸ§© System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Agent   â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚ Proposes tool call
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Senteniel Gateway â”‚  (GraphQL)
â”‚  - Policy Engine â”‚
â”‚  - Risk Scoring  â”‚
â”‚  - GraphRAG      â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚ Allowed
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MCP Server   â”‚  (Sandboxed tools)
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â–¼
 Real / Mock Tools
```

---

## ðŸ”„ How It Works

1. A user or system submits a task  
2. The agent proposes one or more tool calls  
3. Senteniel evaluates:
   - tool permissions
   - role and environment
   - untrusted input boundaries
   - policy constraints
4. GraphRAG retrieves relevant policies and prior incidents  
5. A decision is made and persisted  
6. Approved calls are forwarded to the MCP server  
7. Tool outputs are redacted and returned  

---

## ðŸ—ƒï¸ Audit Database (Core Tables)

- **runs**
- **tool_calls**
- **decisions**

All schema changes are managed via Alembic migrations.
GraphQL read queries are available to fetch **runs**, **tool_calls**, and **decisions** for UI dashboards and leaderboards.

---

## ðŸš€ Quickstart (Docker Compose)

### 1) Start services
```bash
docker compose up -d --build
```

### 2) Health check
```bash
docker compose ps
```

### 3) Try the orchestrators

#### LangGraph
```bash
curl "http://localhost:8000/agent/run?task=list files"
curl "http://localhost:8000/agent/run?task=read /etc/passwd"
```

#### CrewAI
```bash
curl "http://localhost:8000/agent/crew/run?task=list files"
curl "http://localhost:8000/agent/crew/run?task=read /etc/passwd"
```

#### Hybrid FSM
```bash
curl "http://localhost:8000/agent/fsm/run?task=list files"
curl "http://localhost:8000/agent/fsm/run?task=read /etc/passwd"
```

### Environment notes
- Inside Docker, set `GATEWAY_GRAPHQL_URL` to the service DNS if needed (e.g., `http://gateway-api:8000/graphql`).
- If using Ollama for local LLM planning/summaries, set `OLLAMA_BASE_URL=http://ollama:11434`.
- LangGraph includes a deterministic fallback when the LLM is unreachable (no crashes).

---

## ðŸ–¥ï¸ User Interfaces

Senteniel provides a web UI for **security, platform, and infra teams**:

### ðŸ” Dashboard
- active runs
- tool usage overview
- risk distribution

### ðŸ›‘ Approval Queue
- review pending write actions
- inspect diffs and intent
- approve or deny with justification

### ðŸ§¾ Decision Trace View
- full reasoning graph
- policy citations
- incident references

### ðŸ Leaderboard
- LangGraph vs FSM performance comparison
- safety vs utility tradeâ€‘offs

---

## ðŸ›£ï¸ Roadmap

### âœ… Phase 0 â€” System Spine
- Dockerized gateway and MCP server
- GraphQL control plane
- Audit logging
- Prometheus metrics
- Persisted audit logs with GraphQL read queries (runs, tool calls, decisions)

### ðŸš§ Phase 1 â€” Orchestration Comparison
- âœ… LangGraph runner (single-agent) â€” `POST /agent/run?task=...`
- âœ… FSM runner (deterministic baseline) â€” `POST /agent/fsm/run?task=...`
- âœ… Fairness rule enforced: same tools, same policies, same MCP boundary; only orchestrator differs

### ðŸš§ Phase 2 â€” GraphRAG Proof Mode
- Neo4j policy graph
- Incident grounding
- Decision explanation graphs

### ðŸš§ Phase 3 â€” Evaluation Harness
- Promptâ€‘injection test suite
- Leaderboards
- Regression safety tests

---

## ðŸ“Œ Status

**Active development. Designed as a reference architecture for safe agent execution.**

---

## ðŸ¤ Why This Project Matters

Senteniel targets one of the most urgent unsolved problems in modern AI systems:

> *How do we safely allow autonomous agents to act in the real world?*

This repository answers that question with **engineering rigor, explicit safety boundaries, and measurable evaluation** â€” not demos.
