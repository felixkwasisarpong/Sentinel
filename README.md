# ðŸ›¡ï¸ SENTENIEL â€” Secure Control Plane for Toolâ€‘Using AI Agents

![Senteniel Banner](docs/banner.png)

> **A productionâ€‘grade, agentâ€‘safety and toolâ€‘use control platform for autonomous AI systems â€” combining MCPâ€‘based tool isolation, LangGraph and FSM orchestration, GraphRAGâ€‘backed policy reasoning, and auditâ€‘grade decision traces. Designed to evaluate and enforce safe agent execution at scale.**

---

![Python](https://img.shields.io/badge/python-3.11-blue)
![FastAPI](https://img.shields.io/badge/FastAPI-API-green)
![GraphQL](https://img.shields.io/badge/GraphQL-Control_Plane-purple)
![LangGraph](https://img.shields.io/badge/LangGraph-Orchestration-orange)
![FSM](https://img.shields.io/badge/Custom_FSM-Orchestration-lightgrey)
![MCP](https://img.shields.io/badge/MCP-Tool_Boundary-black)
![GraphRAG](https://img.shields.io/badge/GraphRAG-Policy_Reasoning-blue)
![Neo4j](https://img.shields.io/badge/Neo4j-Knowledge_Graph-brightgreen)
![Pinecone](https://img.shields.io/badge/Pinecone-Vector_Search-yellowgreen)
![Postgres](https://img.shields.io/badge/Postgres-Audit_DB-blue)
![Docker](https://img.shields.io/badge/Docker-Compose-blue)
![Status](https://img.shields.io/badge/status-active_development-orange)

---

â­ **Star this repository** â€” Senteniel is built as a reference system for safe, auditable agent execution.

---

## ðŸ”¥ Why Senteniel?

**Senteniel is not a chatbot, copilot, or demo agent.**  
It is a **control plane** for **toolâ€‘using AI agents**.

As modern LLM agents gain the ability to:
- read files
- query logs
- open tickets
- modify repositories
- interact with production systems

**prompt injection and unsafe tool execution become real security risks.**

Senteniel exists to answer a single hard question:

> *â€œShould this agent be allowed to do this â€” and can we prove why?â€*

---

## ðŸ§  About

Senteniel is an **agentâ€‘security research and engineering platform** built to:

- enforce **leastâ€‘privilege tool use**
- prevent **promptâ€‘injectionâ€‘driven actions**
- require **human approval for risky operations**
- produce **auditâ€‘grade reasoning traces**
- compare **LangGraph vs a Custom FSM** under identical safety constraints

It is **industryâ€‘agnostic** and applicable to:
- FAANGâ€‘scale internal tooling
- fintech, infra, and SRE platforms
- enterprise agent frameworks
- AI governance and security research

---

## ðŸ—ï¸ Core Concepts

Senteniel enforces strict separation of concerns:

| Layer | Responsibility |
|------|----------------|
| **Agent** | Proposes actions |
| **Gateway (Senteniel)** | Decides if actions are allowed |
| **MCP Server** | Executes tools (sandboxed) |
| **Policy Engine** | Enforces RBAC / ABAC |
| **GraphRAG** | Grounds decisions in policy and incidents |
| **Audit Store** | Persists every decision |
| **Evaluation Harness** | Compares orchestrators |

---

## âœ¨ Key Features

### ðŸ›¡ï¸ Agent Firewall (Control Plane)
- Intercepts **all agent tool calls**
- Enforces:
  - explicit allowlists
  - roleâ€‘based and environmentâ€‘based access
  - strict input validation
- Blocks unsafe execution **before tools are reached**

---

### ðŸ”Œ MCPâ€‘Based Tool Boundary
- All tools exposed via a **sandboxed MCP server**
- Readâ€‘only by default
- Write actions require explicit human approval
- No direct tool access from agents

This makes toolâ€‘use safety **concrete and enforceable**, not theoretical.

---

### ðŸ§  Dual Orchestration Engines
Senteniel evaluates the **same agent logic** across two orchestration strategies:

- **LangGraph**
- **Custom Finite State Machine (FSM)**

Everything else remains identical:
- LLM
- tools
- policies
- retrieval
- evaluation harness

This enables a **fair, reproducible comparison**.

**Proof (current output):**

#### LangGraph (example)
```json
{
  "user_task": "list files",
  "plan": "Use fs.list_dir to inspect the sandbox",
  "tool_result": "[]",
  "final_answer": "Tool executed successfully. Result: []"
}
```

#### FSM (example)
```json
{
  "final_state": {
    "user_task": "list files",
    "plan": "List sandbox files",
    "tool": "fs.list_dir",
    "args": {
      "path": "/sandbox"
    },
    "decision": "ALLOW",
    "result": "[]"
  }
}
```

---

### ðŸ“š GraphRAGâ€‘Backed Policy Reasoning
- Policies, incidents, and tool contracts stored in a **knowledge graph**
- Relevant subgraphs retrieved at decision time
- Decisions are grounded with:
  - policy citations
  - prior incident references
  - explicit reasoning paths

---

### ðŸ§¾ Auditâ€‘Grade Decision Traces
Every tool proposal produces a durable record:
- decision (ALLOW / BLOCK / APPROVAL_REQUIRED)
- rationale
- policy citations
- risk score
- redacted tool arguments
- timestamps

Nothing is implicit. Nothing is hidden.

---

### ðŸ“Š Evaluation & Leaderboards
Senteniel includes a builtâ€‘in evaluation harness to measure:

- promptâ€‘injection block rate
- unsafe execution rate
- falseâ€‘block rate
- task success rate
- latency
- tool calls per run

Results are compared across:
- LangGraph vs FSM
- different policy strictness profiles

---

## ðŸ§© System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Agent   â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚ Proposes tool call
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Senteniel Gateway â”‚  (GraphQL)
â”‚  - Policy Engine â”‚
â”‚  - Risk Scoring  â”‚
â”‚  - GraphRAG      â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚ Allowed
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MCP Server   â”‚  (Sandboxed tools)
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â–¼
 Real / Mock Tools
```

---

## ðŸ”„ How It Works

1. A user or system submits a task  
2. The agent proposes one or more tool calls  
3. Senteniel evaluates:
   - tool permissions
   - role and environment
   - untrusted input boundaries
   - policy constraints
4. GraphRAG retrieves relevant policies and prior incidents  
5. A decision is made and persisted  
6. Approved calls are forwarded to the MCP server  
7. Tool outputs are redacted and returned  

---

## ðŸ—ƒï¸ Audit Database (Core Tables)

- **runs**
- **tool_calls**
- **decisions**

All schema changes are managed via Alembic migrations.

---

## ðŸ–¥ï¸ User Interfaces

Senteniel provides a web UI for **security, platform, and infra teams**:

### ðŸ” Dashboard
- active runs
- tool usage overview
- risk distribution

### ðŸ›‘ Approval Queue
- review pending write actions
- inspect diffs and intent
- approve or deny with justification

### ðŸ§¾ Decision Trace View
- full reasoning graph
- policy citations
- incident references

### ðŸ Leaderboard
- LangGraph vs FSM performance comparison
- safety vs utility tradeâ€‘offs

---

## ðŸ›£ï¸ Roadmap

### âœ… Phase 0 â€” System Spine
- Dockerized gateway and MCP server
- GraphQL control plane
- Audit logging
- Prometheus metrics
- Persisted audit logs with GraphQL read queries (runs, tool calls, decisions)

### ðŸš§ Phase 1 â€” Orchestration Comparison
- âœ… LangGraph runner (single-agent) â€” `POST /agent/run?task=...`
- âœ… FSM runner (deterministic baseline) â€” `POST /agent/fsm/run?task=...`
- âœ… Fairness rule enforced: same tools, same policies, same MCP boundary; only orchestrator differs

### ðŸš§ Phase 2 â€” GraphRAG Proof Mode
- Neo4j policy graph
- Incident grounding
- Decision explanation graphs

### ðŸš§ Phase 3 â€” Evaluation Harness
- Promptâ€‘injection test suite
- Leaderboards
- Regression safety tests

---

## ðŸ“Œ Status

**Active development. Designed as a reference architecture for safe agent execution.**

---

## ðŸ¤ Why This Project Matters

Senteniel targets one of the most urgent unsolved problems in modern AI systems:

> *How do we safely allow autonomous agents to act in the real world?*

This repository answers that question with **engineering rigor, explicit safety boundaries, and measurable evaluation** â€” not demos.
